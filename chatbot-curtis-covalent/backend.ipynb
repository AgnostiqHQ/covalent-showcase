{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot Backend Server Deployment Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import covalent_cloud as cc\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from covalent_cloud.cloud_executor import GPU_TYPE\n",
    "\n",
    "cc.save_api_key(os.environ[\"CC_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Already Exists.\n"
     ]
    }
   ],
   "source": [
    "ENV_NAME = \"chatbot-demo-backend\"\n",
    "\n",
    "cc.create_env(\n",
    "    name=ENV_NAME,\n",
    "    pip = [\n",
    "        \"accelerate==0.29.1\",\n",
    "        \"sentencepiece==0.2.0\",\n",
    "        \"torch==2.2.2\",\n",
    "        \"transformers==4.39.3\",\n",
    "    ],\n",
    "    wait=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_executor = cc.CloudExecutor(\n",
    "    env=ENV_NAME,\n",
    "    num_cpus=24,\n",
    "    memory=\"54 GB\",\n",
    "    time_limit=\"15 days\",\n",
    "    num_gpus=1,\n",
    "    gpu_type=GPU_TYPE.L40,\n",
    ")\n",
    "\n",
    "@cc.service(executor=gpu_executor, name=\"LLM Chatbot Server\")\n",
    "def chatbot_backend(model_path: str, device_map=\"auto\"):\n",
    "    \"\"\"Create a Llama2 chatbot server.\"\"\"\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map=device_map,\n",
    "        torch_dtype=torch.float16,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "    return {\"pipe\": pipe}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@chatbot_backend.endpoint(\"/generate\", name=\"Generate Response\")\n",
    "def generate(pipe, prompt, max_new_tokens=50):\n",
    "    \"\"\"Generate a response to a prompt.\"\"\"\n",
    "    output = pipe(\n",
    "        prompt, max_new_tokens=max_new_tokens,\n",
    "        do_sample=True, truncation=True, temperature=0.9\n",
    "    )\n",
    "    gen_text = output[0]['generated_text']\n",
    "    return gen_text\n",
    "\n",
    "@chatbot_backend.endpoint(\"/stream\", name=\"Stream Response\", streaming=True)\n",
    "def generate_stream(pipe, prompt, max_new_tokens=200):\n",
    "    \"\"\"Generate a response to a prompt, streaming tokens.\"\"\"\n",
    "\n",
    "    def _starts_with_space(tokenizer, token_id):\n",
    "        token = tokenizer.convert_ids_to_tokens(token_id)\n",
    "        return token.startswith('▁')\n",
    "\n",
    "    model = pipe.model\n",
    "    tokenizer = pipe.tokenizer\n",
    "    _input = tokenizer(prompt, return_tensors='pt').to(\"cuda\")\n",
    "\n",
    "    for output_length in range(max_new_tokens):\n",
    "        # Generate next token\n",
    "        output = model.generate(\n",
    "            **_input, max_new_tokens=1, do_sample=True,\n",
    "            temperature=0.9, pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "        # Check for stopping condition\n",
    "        current_token_id = output[0][-1]\n",
    "        if current_token_id == tokenizer.eos_token_id:\n",
    "            break\n",
    "        # Decode token\n",
    "        current_token = tokenizer.decode(\n",
    "            current_token_id, skip_special_tokens=True\n",
    "        )\n",
    "        if _starts_with_space(tokenizer, current_token_id.item()) and output_length > 1:\n",
    "            current_token = ' ' + current_token\n",
    "\n",
    "        yield current_token\n",
    "\n",
    "        # Update input for next iteration.\n",
    "        # Output grows in size with each iteration.\n",
    "        _input = {\n",
    "            'input_ids': output.to(\"cuda\"),\n",
    "            'attention_mask': torch.ones(1, len(output[0])).to(\"cuda\"),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### When ready, copy backend base URL (address) into streamlit app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╭──────────────────────── Deployment Information ────────────────────────╮\n",
      "│  Name          LLM Chatbot Server                                      │\n",
      "│  Description   Create a Llama2 chatbot server.                         │\n",
      "│  Function ID   66563a64f7d37dbf2a468ca9                                │\n",
      "│  Address       https://fn.prod.covalent.xyz/166563a64f7d37dbf2a468ca9  │\n",
      "│  Status        ACTIVE                                                  │\n",
      "│  Tags                                                                  │\n",
      "│  Auth Enabled  Yes                                                     │\n",
      "╰────────────────────────────────────────────────────────────────────────╯\n",
      "╭─────────────────────────────────────────────────╮\n",
      "│ \u001b[3m                POST /generate                 \u001b[0m │\n",
      "│  Streaming    No                                │\n",
      "│  Description  Generate a response to a prompt.  │\n",
      "╰─────────────────────────────────────────────────╯\n",
      "╭───────────────────────────────────────────────────────────────────╮\n",
      "│ \u001b[3m                          POST /stream                           \u001b[0m │\n",
      "│  Streaming    Yes                                                 │\n",
      "│  Description  Generate a response to a prompt, streaming tokens.  │\n",
      "╰───────────────────────────────────────────────────────────────────╯\n",
      "Authorization token: <redacted-authorization-token>\n",
      "\n",
      "https://fn.prod.covalent.xyz/166563a64f7d37dbf2a468ca9\n"
     ]
    }
   ],
   "source": [
    "info = cc.deploy(chatbot_backend)(model_path=\"NousResearch/Llama-2-7b-chat-hf\")\n",
    "info = cc.get_deployment(info.function_id, wait=True)\n",
    "print(info)\n",
    "print(info.address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Danger Zone!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info.teardown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
